# ===== Hugging Face API Configuration =====
# Get your token from: https://huggingface.co/settings/tokens
# Requires READ access for Inference API
HF_TOKEN=HF_TOKEN_HERE

# K2 Think model identifier
K2_MODEL_ID=LLM360/K2-Think

# ===== Application Settings =====
# Logging level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Enable debug mode (verbose logging, stack traces in UI)
DEBUG_MODE=false

# ===== Performance Tuning =====
# Maximum tokens for K2 Think responses
MAX_TOKENS=512

# Temperature for response generation (0.0-1.0)
TEMPERATURE=0.1

# Request timeout in seconds
REQUEST_TIMEOUT=30

# ===== Testing Configuration =====
# Use mock responses if K2 Think API unavailable
MOCK_MODE=false

# Maximum parallel requests for batch testing
MAX_PARALLEL_REQUESTS=5

# ===== Dataset Configuration =====
# Number of samples to load from JailbreakBench
JAILBREAK_BENCH_SAMPLES=50

# Number of samples to load from LLMail-Inject
LLMAIL_INJECT_SAMPLES=30

# ===== Deployment Settings =====
# Set to 'production' on Hugging Face Spaces
ENVIRONMENT=development

# Streamlit server port (default: 8501)
PORT=8501
